{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "chapter_010.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-_VdUhAR5v6c"
      },
      "source": [
        "# Text Autocompletion with LSTM and Beam Search"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gY0lE7i77RyI"
      },
      "source": [
        "This notebook uses the text from the book, [Frankenstein; Or, The Modern Prometheus by Mary Wollstonecraft Shelley](https://www.gutenberg.org/ebooks/42324) as the training dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j-dzPSy75kzA"
      },
      "source": [
        "import numpy as np\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.layers import LSTM \n",
        "import tensorflow as tf\n",
        "import logging\n",
        "tf.get_logger().setLevel(logging.ERROR)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RcEk_G207_vh"
      },
      "source": [
        "EPOCHS = 32\n",
        "BATCH_SIZE = 256\n",
        "INPUT_FILE_NAME = 'frankenstein.txt'\n",
        "WINDOW_LENGTH = 40\n",
        "WINDOW_STEP = 3\n",
        "BEAM_SIZE = 8\n",
        "NUM_LETTERS = 11\n",
        "MAX_LENGTH = 50"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6IttuWz68S09"
      },
      "source": [
        "# Open the text file\n",
        "file = open(INPUT_FILE_NAME, 'r', encoding='utf-8-sig') # Decode with BOM\n",
        "text = file.read()\n",
        "file.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0SD5o5T6_If2"
      },
      "source": [
        "# Convert all text to lowercase,\n",
        "# strip newlines and extra spaces.\n",
        "text = text.lower().strip()\n",
        "text = text.replace('\\n', ' ')\n",
        "text = text.replace('  ', ' ')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e9a9LqR0_XdW"
      },
      "source": [
        "# Encode characters as indices\n",
        "unique_chars = list(set(text))\n",
        "char_to_index = dict((ch, index) for index, ch in enumerate(unique_chars))\n",
        "index_to_char = dict((index, ch) for index, ch in enumerate(unique_chars))\n",
        "encoding_width = len(char_to_index)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yp3xOFXJ_YOZ"
      },
      "source": [
        "len(unique_chars), len(char_to_index)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wGYWMYzPBTc1"
      },
      "source": [
        "# Create the training set\n",
        "fragments = []\n",
        "targets = []\n",
        "\n",
        "# Subtract the WINDOW_LENGTH to allow extraction just to the last\n",
        "# valid range.\n",
        "for i in range(0, len(text) - WINDOW_LENGTH, WINDOW_STEP):\n",
        "  fragments.append(text[i: i + WINDOW_LENGTH])\n",
        "  targets.append(text[i + WINDOW_LENGTH])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "53QDTkWZBro_"
      },
      "source": [
        "# Examples \n",
        "fragments[20001], targets[20001]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pPmgkNsBFV5J"
      },
      "source": [
        "# Convert to one-hot encoded training data.\n",
        "X = np.zeros((len(fragments), WINDOW_LENGTH, encoding_width))\n",
        "y = np.zeros((len(fragments), encoding_width))\n",
        "for i, fragment in enumerate(fragments):\n",
        "  for j, char in enumerate(fragment):\n",
        "    X[i, j, char_to_index[char]] = 1\n",
        "    target_char = targets[i]\n",
        "    y[i, char_to_index[target_char]] = 1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fkQJ13ZBGbsa"
      },
      "source": [
        "# Building the model for training.\n",
        "model = Sequential()\n",
        "model.add(LSTM(128, return_sequences=True, dropout=0.2, recurrent_dropout=0.2, input_shape=(None, encoding_width)))\n",
        "model.add(LSTM(128, dropout=0.2, recurrent_dropout=0.2))\n",
        "model.add(Dense(encoding_width, activation='softmax'))\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xf-ji-fwJe57"
      },
      "source": [
        "# Training,\n",
        "# validation split set to 5%, good performance is highly subjective here.\n",
        "history = model.fit(X, y, validation_split=0.05, batch_size=BATCH_SIZE, epochs=EPOCHS, verbose=2, shuffle=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uOOF7U_cJwWq"
      },
      "source": [
        "# Beam search implementation\n",
        "# Create initial single beam represented by triplet.\n",
        "# (probability, string, one-hot encoded string)\n",
        "letters = 'the man '\n",
        "one_hots = []\n",
        "for i, char in enumerate(letters):\n",
        "  x = np.zeros(encoding_width)\n",
        "  x[char_to_index[char]] = 1\n",
        "  one_hots.append(x)\n",
        "beams = [(np.log(1.0), letters, one_hots)]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cZhb8AwyYQW6"
      },
      "source": [
        "# Predict NUM_LETTERS into the future.\n",
        "for i in range(NUM_LETTERS):\n",
        "  minibatch_list = []\n",
        "  # Create minibatch from one-hot encodings, and predict.\n",
        "  for triple in beams:\n",
        "    # Extract the one-hot enconding representation of each beam.\n",
        "    minibatch_list.append(triple[2])\n",
        "  minibatch = np.array(minibatch_list)\n",
        "  # Predict the one-hot encode for potential next characters.\n",
        "  y_predict = model.predict(minibatch, verbose=0)\n",
        "\n",
        "  new_beams = []\n",
        "  for j, softmax_vec in enumerate(y_predict):\n",
        "    triple = beams[j]\n",
        "    # Create BEAM_SIZE new beams from each existing beam.\n",
        "    for k in range(BEAM_SIZE):\n",
        "      # Get the probable character.\n",
        "      char_index = np.argmax(softmax_vec)\n",
        "      # Calculate the new probability.\n",
        "      new_prob = triple[0] + np.log(softmax_vec[char_index])\n",
        "      # Add new letter to the string.\n",
        "      new_letters = triple[1] + index_to_char[char_index]\n",
        "      # Encode the new character.\n",
        "      x = np.zeros(encoding_width)\n",
        "      x[char_index] = 1\n",
        "      # Create a copy of the old one_hot encoded representation,\n",
        "      # and append the new character (it's one-hot encoded version.)\n",
        "      new_one_hots = triple[2].copy()\n",
        "      new_one_hots.append(x)\n",
        "      # Append the new beam to the list of beams.\n",
        "      new_beams.append((new_prob, new_letters, new_one_hots))\n",
        "      # Set that index to zero, so that np.argmax can find,\n",
        "      # the next probable character. \n",
        "      softmax_vec[char_index] = 0\n",
        "  # Prune tree to only keep BEAM_SIZE most probable beams.\n",
        "  new_beams.sort(key=lambda tup: tup[0], reverse=True)\n",
        "  beams = new_beams[0:BEAM_SIZE]\n",
        "\n",
        "for item in beams:\n",
        "  print(item[1])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2QXNUgoclVke"
      },
      "source": [
        "PYTORCH"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lJ7V35adlWVm"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "from sklearn.model_selection import train_test_split\n",
        "import numpy as np\n",
        "from utils import train_model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eG0pHq-YmgAA"
      },
      "source": [
        "# Device setup\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qyoLqivFm44U"
      },
      "source": [
        "# Training parameters reused as above.\n",
        "# Create training examples.\n",
        "fragments = []\n",
        "targets = []\n",
        "for i in range(0, len(text) - WINDOW_LENGTH, WINDOW_STEP):\n",
        "    fragments.append(text[i: i + WINDOW_LENGTH])\n",
        "    targets.append(text[i + WINDOW_LENGTH])\n",
        "\n",
        "# Convert to one-hot encoded training data.\n",
        "X = np.zeros((len(fragments), WINDOW_LENGTH, encoding_width), dtype=np.float32)\n",
        "y = np.zeros(len(fragments), dtype=np.int64)\n",
        "for i, fragment in enumerate(fragments):\n",
        "    for j, char in enumerate(fragment):\n",
        "        X[i, j, char_to_index[char]] = 1\n",
        "    target_char = targets[i]\n",
        "    y[i] = char_to_index[target_char]\n",
        "    \n",
        "# Train test split\n",
        "train_X, test_X, train_y, test_y = train_test_split(\n",
        "    X, y, test_size=0.05, random_state=0\n",
        ")\n",
        "\n",
        "# Create dataset objects\n",
        "trainset = TensorDataset(torch.from_numpy(train_X), torch.from_numpy(train_y))\n",
        "testset = TensorDataset(torch.from_numpy(test_X), torch.from_numpy(test_y))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MRVC7VJ6rxZy"
      },
      "source": [
        "# trainset.dtype"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zqq4TkllngSs"
      },
      "source": [
        "# Custom layer to handle the output of the recurrent neural network.\n",
        "class LastTimestep(nn.Module):\n",
        "  def forward(self, inputs):\n",
        "    return inputs[1][0][1]  # Return hidden state and not the cell state of the last timestep."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rHPae6hdpfV9"
      },
      "source": [
        "# The model\n",
        "model = nn.Sequential(\n",
        "    nn.LSTM(encoding_width, 128, num_layers=2, dropout=0.2, batch_first=True),\n",
        "    LastTimestep(),\n",
        "    nn.Dropout(0.2),\n",
        "    nn.Linear(128, encoding_width)\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KDPiZjyoqEO6"
      },
      "source": [
        "model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rREtSW3yqFhV"
      },
      "source": [
        "# Loss function and optimizer\n",
        "optimizer = torch.optim.Adam(model.parameters())\n",
        "loss_function = nn.CrossEntropyLoss()\n",
        "\n",
        "# Training\n",
        "train_model(model, device, EPOCHS, BATCH_SIZE, trainset, testset, optimizer, loss_function, 'acc')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NmzVBVQtqfOa"
      },
      "source": [
        "# Create initial single beam represented by triplet\n",
        "# (probability , string , one-hot encoded string).\n",
        "# Generally the same as Tensorflow just a little more involved.\n",
        "letters = 'i trembled '\n",
        "one_hots = []\n",
        "for i, char in enumerate(letters):\n",
        "    x = np.zeros(encoding_width)\n",
        "    x[char_to_index[char]] = 1\n",
        "    one_hots.append(x)\n",
        "beams = [(np.log(1.0), letters, one_hots)]\n",
        "\n",
        "# Predict NUM_LETTERS into the future.\n",
        "for i in range(NUM_LETTERS):\n",
        "    minibatch_list = []\n",
        "    # Create minibatch from one-hot encodings, and predict.\n",
        "    for triple in beams:\n",
        "        minibatch_list.append(triple[2])\n",
        "    minibatch = np.array(minibatch_list, dtype=np.float32)\n",
        "\n",
        "    # A quite more involved prediction.\n",
        "    # Convert to pytorch tensors.\n",
        "    inputs = torch.from_numpy(minibatch)\n",
        "    # Send the input to the same device as the model.\n",
        "    inputs = inputs.to(device)\n",
        "    # Perform the prediction.\n",
        "    outputs = model(inputs)\n",
        "    # Run the output through the softmax to get character probabilities.\n",
        "    outputs = F.softmax(outputs, dim=1)\n",
        "    # Return the output to the cpu.\n",
        "    y_predict = outputs.cpu().detach().numpy()\n",
        "\n",
        "    new_beams = []\n",
        "    for j, softmax_vec in enumerate(y_predict):\n",
        "        triple = beams[j]\n",
        "        # Create BEAM_SIZE new beams from each existing beam.\n",
        "        for k in range(BEAM_SIZE):\n",
        "            char_index = np.argmax(softmax_vec)\n",
        "            new_prob = triple[0] + np.log(softmax_vec[char_index])\n",
        "            new_letters = triple[1] + index_to_char[char_index]\n",
        "            x = np.zeros(encoding_width)\n",
        "            x[char_index] = 1\n",
        "            new_one_hots = triple[2].copy()\n",
        "            new_one_hots.append(x)\n",
        "            new_beams.append((new_prob, new_letters, new_one_hots))\n",
        "            softmax_vec[char_index] = 0\n",
        "    # Prune tree to only keep BEAM_SIZE most probable beams.\n",
        "    new_beams.sort(key=lambda tup: tup[0], reverse=True)\n",
        "    beams = new_beams[0:BEAM_SIZE]\n",
        "for item in beams:\n",
        "    print(item[1])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iUh7S-_vtzeh"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}
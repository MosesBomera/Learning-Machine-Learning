{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6SsuMB_IAB_u"
      },
      "source": [
        "# Sequence-to-Sequence Networks and Natural Language Translation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6TG1Mc8g_6L2"
      },
      "outputs": [],
      "source": [
        "# Keras functional API.\n",
        "from tensorflow.keras.layers import Input, Dense\n",
        "from tensorflow.keras.models import Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dXWIycK5AcE4",
        "outputId": "832777a1-88b8-47f0-ff5f-76d852d8c09c"
      },
      "outputs": [],
      "source": [
        "# Declare inputs.\n",
        "inputs = Input(shape=(10,))\n",
        "\n",
        "# Declare layers.\n",
        "layer1 = Dense(64, activation='relu')\n",
        "layer2 = Dense(64, activation='relu')\n",
        "\n",
        "# Connect inputs and layers.\n",
        "layer1_outputs = layer1(inputs)\n",
        "layer2_outputs = layer2(layer1_outputs)\n",
        "\n",
        "# Create model.\n",
        "model = Model(inputs=inputs, outputs=layer2_outputs)\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6ytTZhjQBGbs",
        "outputId": "7e13ff18-2b15-4f6d-a138-225fe825c003"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.layers import Input, Dense\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Concatenate\n",
        "\n",
        "# Declare inputs.\n",
        "inputs = Input(shape=(10,))\n",
        "bypass_inputs = Input(shape=(5,))\n",
        "\n",
        "# Declare layers.\n",
        "layer1 = Dense(64, activation='relu')\n",
        "concat_layer = Concatenate()\n",
        "layer2 = Dense(64, activation='relu')\n",
        "\n",
        "# Connect inputs and layers.\n",
        "layer1_outputs = layer1(inputs)\n",
        "layer2_inputs = concat_layer([layer1_outputs, bypass_inputs])\n",
        "layer2_outputs = layer2(layer2_inputs)\n",
        "\n",
        "# Create model.\n",
        "model = Model(inputs=[inputs, bypass_inputs],\n",
        "outputs=layer2_outputs)\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HEphUKwXEOBJ"
      },
      "source": [
        "### Neural Machine Translation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7X3F5vQiJc5_",
        "outputId": "3002f893-b134-43c9-a741-86578bdbe15b"
      },
      "outputs": [],
      "source": [
        "# Download dataset, French to English.\n",
        "!wget http://www.manythings.org/anki/fra-eng.zip\n",
        "\n",
        "# Data directory.\n",
        "!mkdir data\n",
        "\n",
        "# Unzip into data directory.\n",
        "!unzip fra-eng.zip -d data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cBxcLZnbDBs2"
      },
      "outputs": [],
      "source": [
        "# Libraries\n",
        "import numpy as np\n",
        "import random\n",
        "from tensorflow.keras.layers import Input\n",
        "from tensorflow.keras.layers import Embedding\n",
        "from tensorflow.keras.layers import LSTM\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.optimizers import RMSprop\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.text import text_to_word_sequence\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "import tensorflow as tf \n",
        "import logging\n",
        "tf.get_logger().setLevel(logging.ERROR)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OaSltHfOFJ-o"
      },
      "outputs": [],
      "source": [
        "# Constants\n",
        "EPOCHS = 20\n",
        "BATCH_SIZE = 128\n",
        "MAX_WORDS = 10000\n",
        "READ_LINES = 60000\n",
        "LAYER_SIZE = 256\n",
        "EMBEDDING_WIDTH = 128\n",
        "TEST_PERCENT = 0.2\n",
        "SAMPLE_SIZE = 20\n",
        "OOV_WORD = 'UNK'\n",
        "PAD_INDEX = 0\n",
        "OOV_INDEX = 1\n",
        "START_INDEX = MAX_WORDS - 2\n",
        "STOP_INDEX = MAX_WORDS - 1\n",
        "MAX_LENGTH = 60\n",
        "SRC_DEST_FILE_NAME = 'data/fra.txt'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b2eX7P75KRTH"
      },
      "outputs": [],
      "source": [
        "# Read in the file.\n",
        "def read_file_combined(file_name, max_len):\n",
        "  file = open(file_name, 'r', encoding='utf-8')\n",
        "  src_word_sequences = []\n",
        "  dest_word_sequences = []\n",
        "  for i, line in enumerate(file):\n",
        "    if i == READ_LINES:\n",
        "      break\n",
        "    pair = line.split('\\t')\n",
        "    word_sequence = text_to_word_sequence(pair[1])\n",
        "    src_word_sequence = word_sequence[0:max_len]\n",
        "    src_word_sequences.append(src_word_sequence)\n",
        "    word_sequence = text_to_word_sequence(pair[0])\n",
        "    dest_word_sequence = word_sequence[0:max_len]\n",
        "    dest_word_sequences.append(dest_word_sequence)\n",
        "  file.close()\n",
        "  return src_word_sequences, dest_word_sequences"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2oI-dznbMxmG"
      },
      "outputs": [],
      "source": [
        "# Functions to tokenize and un-tokenize sequences.\n",
        "def tokenize(sequences):\n",
        "    # \"MAX_WORDS-2\" used to reserve two indices\n",
        "    # for START and STOP.\n",
        "    tokenizer = Tokenizer(num_words=MAX_WORDS-2,\n",
        "                          oov_token=OOV_WORD)\n",
        "    tokenizer.fit_on_texts(sequences)\n",
        "    token_sequences = tokenizer.texts_to_sequences(sequences)\n",
        "    return tokenizer, token_sequences\n",
        "\n",
        "def tokens_to_words(tokenizer, seq):\n",
        "    word_seq = []\n",
        "    for index in seq:\n",
        "        if index == PAD_INDEX:\n",
        "            word_seq.append('PAD')\n",
        "        elif index == OOV_INDEX:\n",
        "            word_seq.append(OOV_WORD)\n",
        "        elif index == START_INDEX:\n",
        "            word_seq.append('START')\n",
        "        elif index == STOP_INDEX:\n",
        "            word_seq.append('STOP')\n",
        "        else:\n",
        "            word_seq.append(tokenizer.sequences_to_texts(\n",
        "                [[index]])[0])\n",
        "    print(word_seq)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ms8puNFkOVKe"
      },
      "outputs": [],
      "source": [
        "# Read file and tokenize.\n",
        "src_seq, dest_seq = read_file_combined(SRC_DEST_FILE_NAME,MAX_LENGTH)\n",
        "src_tokenizer, src_token_seq = tokenize(src_seq)\n",
        "dest_tokenizer, dest_token_seq = tokenize(dest_seq)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zmM5sDsaOdje",
        "outputId": "12af8851-f4cf-464a-b57f-bfaa40fa4c90"
      },
      "outputs": [],
      "source": [
        "# Example\n",
        "src_seq[1010], dest_seq[1010]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9zuhgRmbOipz",
        "outputId": "4ec1139c-7b1a-4456-caba-2bb109a776ca"
      },
      "outputs": [],
      "source": [
        "src_token_seq[1010], dest_token_seq[1010]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mYCuAsKckXgu"
      },
      "outputs": [],
      "source": [
        "# Prepare training data.\n",
        "dest_target_token_seq = [x + [STOP_INDEX] for x in dest_token_seq]\n",
        "dest_input_token_seq = [[START_INDEX] + x for x in dest_target_token_seq]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZSoB26nynXmG"
      },
      "outputs": [],
      "source": [
        "# Prepadding for source input data.\n",
        "src_input_data = pad_sequences(src_token_seq)\n",
        "# Postpadding for the destination input data.\n",
        "dest_input_data = pad_sequences(dest_input_token_seq, padding='post')\n",
        "# Post padding for the target data.\n",
        "dest_target_data = pad_sequences( dest_target_token_seq, padding='post', maxlen=len(dest_input_data[0]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v2qU6R5-nxjM",
        "outputId": "2cc4edc8-4af9-4208-f0be-57a7fafd08fc"
      },
      "outputs": [],
      "source": [
        "# Padded data\n",
        "src_input_data[-1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Od4j20jRoKAh"
      },
      "outputs": [],
      "source": [
        "# Train test split\n",
        "rows = len(src_input_data[:,0])\n",
        "all_indices = list(range(rows))\n",
        "test_rows = int(rows * TEST_PERCENT)\n",
        "test_indices = random.sample(all_indices, test_rows)\n",
        "train_indices = [x for x in all_indices if x not in test_indices]\n",
        "\n",
        "train_src_input_data = src_input_data[train_indices]\n",
        "train_dest_input_data = dest_input_data[train_indices]\n",
        "train_dest_target_data = dest_target_data[train_indices]\n",
        "\n",
        "test_src_input_data = src_input_data[test_indices]\n",
        "test_dest_input_data = dest_input_data[test_indices]\n",
        "test_dest_target_data = dest_target_data[test_indices]\n",
        "\n",
        "# Create a sample of the test set that we will inspect in detail.\n",
        "test_indices = list(range(test_rows))\n",
        "sample_indices = random.sample(test_indices, SAMPLE_SIZE)\n",
        "sample_input_data = test_src_input_data[sample_indices]\n",
        "sample_target_data = test_dest_target_data[sample_indices]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OZvIikBppyhY",
        "outputId": "fbe2c6e5-c946-4271-9a51-94a9e08cabf2"
      },
      "outputs": [],
      "source": [
        "# The model\n",
        "# Input is input sequence in source language.\n",
        "enc_embedding_input = Input(shape=(None, ))\n",
        "\n",
        "# Create the encoder layers.\n",
        "enc_embedding_layer = Embedding(output_dim=EMBEDDING_WIDTH, input_dim=MAX_WORDS, mask_zero=True)\n",
        "enc_layer1 = LSTM(LAYER_SIZE, return_state=True, return_sequences=True)\n",
        "enc_layer2 = LSTM(LAYER_SIZE, return_state=True)\n",
        "\n",
        "# Connect the encoder layers.\n",
        "enc_embedding_layer_outputs = enc_embedding_layer(enc_embedding_input)\n",
        "enc_layer1_outputs, enc_layer1_state_h, enc_layer1_state_c = \\\n",
        "              enc_layer1(enc_embedding_layer_outputs)\n",
        "_, enc_layer2_state_h, enc_layer2_state_c = \\\n",
        "          enc_layer2(enc_layer1_outputs)\n",
        "\n",
        "# Build the model.\n",
        "enc_model = Model(\n",
        "    enc_embedding_input, \n",
        "    [enc_layer1_state_h, enc_layer1_state_c, enc_layer2_state_h, enc_layer2_state_c])\n",
        "enc_model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rV8180Z2tFJh",
        "outputId": "12cb2f7c-1094-4ece-dc07-f1d5baf4da95"
      },
      "outputs": [],
      "source": [
        "# The decoder model.\n",
        "# language and intermediate state.\n",
        "dec_layer1_state_input_h = Input(shape=(LAYER_SIZE,))\n",
        "dec_layer1_state_input_c = Input(shape=(LAYER_SIZE,))\n",
        "dec_layer2_state_input_h = Input(shape=(LAYER_SIZE,))\n",
        "dec_layer2_state_input_c = Input(shape=(LAYER_SIZE,))\n",
        "dec_embedding_input = Input(shape=(None, ))\n",
        "\n",
        "# Create the decoder layers.\n",
        "dec_embedding_layer = Embedding(output_dim=EMBEDDING_WIDTH,\n",
        "                                input_dim=MAX_WORDS,\n",
        "                                mask_zero=True)\n",
        "dec_layer1 = LSTM(LAYER_SIZE, return_state = True,\n",
        "                  return_sequences=True)\n",
        "dec_layer2 = LSTM(LAYER_SIZE, return_state = True,\n",
        "                  return_sequences=True)\n",
        "dec_layer3 = Dense(MAX_WORDS, activation='softmax')\n",
        "\n",
        "# Connect the decoder layers.\n",
        "dec_embedding_layer_outputs = dec_embedding_layer(dec_embedding_input)\n",
        "dec_layer1_outputs, dec_layer1_state_h, dec_layer1_state_c = \\\n",
        "    dec_layer1(dec_embedding_layer_outputs,\n",
        "    initial_state=[dec_layer1_state_input_h,\n",
        "                   dec_layer1_state_input_c])\n",
        "dec_layer2_outputs, dec_layer2_state_h, dec_layer2_state_c = \\\n",
        "    dec_layer2(dec_layer1_outputs,\n",
        "    initial_state=[dec_layer2_state_input_h,\n",
        "                   dec_layer2_state_input_c])\n",
        "dec_layer3_outputs = dec_layer3(dec_layer2_outputs)\n",
        "\n",
        "# Build the model.\n",
        "dec_model = Model([dec_embedding_input,\n",
        "                   dec_layer1_state_input_h,\n",
        "                   dec_layer1_state_input_c,\n",
        "                   dec_layer2_state_input_h,\n",
        "                   dec_layer2_state_input_c],\n",
        "                  [dec_layer3_outputs, dec_layer1_state_h,\n",
        "                   dec_layer1_state_c, dec_layer2_state_h,\n",
        "                   dec_layer2_state_c])\n",
        "dec_model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wb2Oj9CRunlh",
        "outputId": "7d66a3b5-8c51-4cb4-fb73-d4442eeb27dc"
      },
      "outputs": [],
      "source": [
        "# Build and compile full training model.\n",
        "# We do not use the state output when training.\n",
        "train_enc_embedding_input = Input(shape=(None, ))\n",
        "train_dec_embedding_input = Input(shape=(None, ))\n",
        "intermediate_state = enc_model(train_enc_embedding_input)\n",
        "train_dec_output, _, _, _, _ = dec_model(\n",
        "    [train_dec_embedding_input] +\n",
        "    intermediate_state)\n",
        "training_model = Model([train_enc_embedding_input,\n",
        "                        train_dec_embedding_input],\n",
        "                        train_dec_output)\n",
        "optimizer = RMSprop(learning_rate=0.01)\n",
        "training_model.compile(loss='sparse_categorical_crossentropy',\n",
        "                       optimizer=optimizer, metrics =['accuracy'])\n",
        "training_model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8K3cE9H5xLpf",
        "outputId": "ebf8825a-89be-4d18-8c38-e19088fe4f03"
      },
      "outputs": [],
      "source": [
        "# Train and test repeatedly.\n",
        "for i in range(EPOCHS):\n",
        "    print('step: ' , i)\n",
        "    # Train model for one epoch.\n",
        "    history = training_model.fit(\n",
        "        [train_src_input_data, train_dest_input_data],\n",
        "        train_dest_target_data, validation_data=(\n",
        "            [test_src_input_data, test_dest_input_data],\n",
        "            test_dest_target_data), batch_size=BATCH_SIZE,\n",
        "        epochs=1)\n",
        "\n",
        "    # Loop through samples to see result\n",
        "    for (test_input, test_target) in zip(sample_input_data,\n",
        "                                         sample_target_data):\n",
        "        # Run a single sentence through encoder model.\n",
        "        x = np.reshape(test_input, (1, -1))\n",
        "        last_states = enc_model.predict(\n",
        "            x, verbose=0)\n",
        "        # Provide resulting state and START_INDEX as input\n",
        "        # to decoder model.\n",
        "        prev_word_index = START_INDEX\n",
        "        produced_string = ''\n",
        "        pred_seq = []\n",
        "        for j in range(MAX_LENGTH):\n",
        "            x = np.reshape(np.array(prev_word_index), (1, 1))\n",
        "            # Predict next word and capture internal state.\n",
        "            preds, dec_layer1_state_h, dec_layer1_state_c, \\\n",
        "                dec_layer2_state_h, dec_layer2_state_c = \\\n",
        "                    dec_model.predict(\n",
        "                        [x] + last_states, verbose=0)\n",
        "            last_states = [dec_layer1_state_h,\n",
        "                           dec_layer1_state_c,\n",
        "                           dec_layer2_state_h,\n",
        "                           dec_layer2_state_c]\n",
        "            # Find the most probable word.\n",
        "            prev_word_index = np.asarray(preds[0][0]).argmax()\n",
        "            pred_seq.append(prev_word_index)\n",
        "            if prev_word_index == STOP_INDEX:\n",
        "                break\n",
        "        tokens_to_words(src_tokenizer, test_input)\n",
        "        tokens_to_words(dest_tokenizer, test_target)\n",
        "        tokens_to_words(dest_tokenizer, pred_seq)\n",
        "        print('\\n\\n')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WN6OadMYxol-"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "chapter_014.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}

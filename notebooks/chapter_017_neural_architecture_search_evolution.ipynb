{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "chapter_017_neural_architecture_search_evolution.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JxXLcBD5Z6Se"
      },
      "source": [
        "# Neural Architecture Search - Evolution Algorithm"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1JMmActsZucT"
      },
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Lambda\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.layers import Flatten\n",
        "from tensorflow.keras.layers import Conv2D\n",
        "from tensorflow.keras.layers import Dropout\n",
        "from tensorflow.keras.layers import MaxPooling2D\n",
        "import numpy as np\n",
        "import logging\n",
        "import copy\n",
        "tf.get_logger().setLevel(logging.ERROR)"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OildsXudaBau"
      },
      "source": [
        "MAX_MODEL_SIZE = 500000\n",
        "CANDIDATE_EVALUATIONS = 500\n",
        "EVAL_EPOCHS = 3\n",
        "FINAL_EPOCHS = 20\n",
        "POPULATION_SIZE = 50"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uI0ivolDaFkz"
      },
      "source": [
        "layer_types = ['DENSE', 'CONV2D', 'MAXPOOL2D']\n",
        "param_values = dict([('size', [16, 64, 256, 1024, 4096]),\n",
        "                ('activation', ['relu', 'tanh', 'elu']),\n",
        "                ('kernel_size', [(1, 1), (2, 2), (3, 3), (4, 4)]),\n",
        "                ('stride', [(1, 1), (2, 2), (3, 3), (4, 4)]),\n",
        "                ('dropout', [0.0, 0.4, 0.7, 0.9])])\n",
        "\n",
        "layer_params = dict([('DENSE', ['size', 'activation', 'dropout']),\n",
        "                     ('CONV2D', ['size', 'activation',\n",
        "                                 'kernel_size', 'stride',\n",
        "                                 'dropout']),\n",
        "                     ('MAXPOOL2D', ['kernel_size', 'stride',\n",
        "                                    'dropout'])])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wMw5m_YnaIqx"
      },
      "source": [
        "# Load dataset.\n",
        "cifar_dataset = keras.datasets.cifar10\n",
        "(train_images, train_labels), (test_images,\n",
        "                    test_labels) = cifar_dataset.load_data()\n",
        "\n",
        "# Standardize dataset.\n",
        "mean = np.mean(train_images)\n",
        "stddev = np.std(train_images)\n",
        "train_images = (train_images - mean) / stddev\n",
        "test_images = (test_images - mean) / stddev\n",
        "\n",
        "# Change labels to one-hot.\n",
        "train_labels = to_categorical(train_labels,\n",
        "                              num_classes=10)\n",
        "test_labels = to_categorical(test_labels,\n",
        "                             num_classes=10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FIL7_JFyaRHI"
      },
      "source": [
        "# Methods to create a model definition.\n",
        "def generate_random_layer(layer_type):\n",
        "    layer = {}\n",
        "    layer['layer_type'] = layer_type\n",
        "    params = layer_params[layer_type]\n",
        "    for param in params:\n",
        "        values = param_values[param]\n",
        "        layer[param] = values[np.random.randint(0, len(values))]\n",
        "    return layer\n",
        "\n",
        "def generate_model_definition():\n",
        "    layer_count = np.random.randint(2, 9)\n",
        "    non_dense_count = np.random.randint(1, layer_count)\n",
        "    layers = []\n",
        "    for i in range(layer_count):\n",
        "        if i < non_dense_count:\n",
        "            layer_type = layer_types[np.random.randint(1, 3)]\n",
        "            layer = generate_random_layer(layer_type)\n",
        "        else:\n",
        "            layer = generate_random_layer('DENSE')\n",
        "        layers.append(layer)\n",
        "    return layers\n",
        "\n",
        "def compute_weight_count(layers):\n",
        "    last_shape = (32, 32, 3)\n",
        "    total_weights = 0\n",
        "    for layer in layers:\n",
        "        layer_type = layer['layer_type']\n",
        "        if layer_type == 'DENSE':\n",
        "            size = layer['size']\n",
        "            weights = size * (np.prod(last_shape) + 1)\n",
        "            last_shape = (layer['size'])\n",
        "        else:\n",
        "            stride = layer['stride']\n",
        "            if layer_type == 'CONV2D':\n",
        "                size = layer['size']\n",
        "                kernel_size = layer['kernel_size']\n",
        "                weights = size * ((np.prod(kernel_size) *\n",
        "                                   last_shape[2]) + 1)\n",
        "                last_shape = (np.ceil(last_shape[0]/stride[0]),\n",
        "                              np.ceil(last_shape[1]/stride[1]),\n",
        "                              size)\n",
        "            elif layer_type == 'MAXPOOL2D':\n",
        "                weights = 0\n",
        "                last_shape = (np.ceil(last_shape[0]/stride[0]),\n",
        "                              np.ceil(last_shape[1]/stride[1]),\n",
        "                              last_shape[2])\n",
        "        total_weights += weights\n",
        "    total_weights += ((np.prod(last_shape) + 1) * 10)\n",
        "    return total_weights\n",
        "\n",
        "# Methods to create and evaluate model based on model definition.\n",
        "def add_layer(model, params, prior_type):\n",
        "    layer_type = params['layer_type']\n",
        "    if layer_type == 'DENSE':\n",
        "        if prior_type != 'DENSE':\n",
        "            model.add(Flatten())\n",
        "        size = params['size']\n",
        "        act = params['activation']\n",
        "        model.add(Dense(size, activation=act))\n",
        "    elif layer_type == 'CONV2D':\n",
        "        size = params['size']\n",
        "        act = params['activation']\n",
        "        kernel_size = params['kernel_size']\n",
        "        stride = params['stride']\n",
        "        model.add(Conv2D(size, kernel_size, activation=act,\n",
        "                         strides=stride, padding='same'))\n",
        "    elif layer_type == 'MAXPOOL2D':\n",
        "        kernel_size = params['kernel_size']\n",
        "        stride = params['stride']\n",
        "        model.add(MaxPooling2D(pool_size=kernel_size,\n",
        "                               strides=stride, padding='same'))\n",
        "    dropout = params['dropout']\n",
        "    if(dropout > 0.0):\n",
        "        model.add(Dropout(dropout))\n",
        "\n",
        "def create_model(layers):\n",
        "    tf.keras.backend.clear_session()\n",
        "    model = Sequential()\n",
        "    model.add(Lambda(lambda x: x, input_shape=(32, 32, 3)))\n",
        "    prev_layer = 'LAMBDA' # Dummy layer to set input_shape\n",
        "    for layer in layers:\n",
        "        add_layer(model, layer, prev_layer)\n",
        "        prev_layer = layer['layer_type']\n",
        "    model.add(Dense(10, activation='softmax'))\n",
        "    model.compile(loss='categorical_crossentropy',\n",
        "                  optimizer='adam', metrics=['accuracy'])\n",
        "    return model\n",
        "\n",
        "def create_and_evaluate_model(model_definition):\n",
        "    weight_count = compute_weight_count(model_definition)\n",
        "    if weight_count > MAX_MODEL_SIZE:\n",
        "        return 0.0\n",
        "    model = create_model(model_definition)\n",
        "    history = model.fit(train_images, train_labels,\n",
        "                        validation_data=(test_images, test_labels),\n",
        "                        epochs=EVAL_EPOCHS, batch_size=64,\n",
        "                        verbose=2, shuffle=False)\n",
        "    acc = history.history['val_accuracy'][-1]\n",
        "    print('Size: ', weight_count)\n",
        "    print('Accuracy: %5.2f' %acc)\n",
        "    return acc\n",
        "\n",
        "# Helper method for hill climbing and evolutionary algorithm.\n",
        "def tweak_model(model_definition):\n",
        "    layer_num = np.random.randint(0, len(model_definition))\n",
        "    last_layer = len(model_definition) - 1\n",
        "    for first_dense, layer in enumerate(model_definition):\n",
        "        if layer['layer_type'] == 'DENSE':\n",
        "            break\n",
        "    if np.random.randint(0, 2) == 1:\n",
        "        delta = 1\n",
        "    else:\n",
        "        delta = -1\n",
        "    if np.random.randint(0, 2) == 1:\n",
        "        # Add/remove layer.\n",
        "        if len(model_definition) < 3:\n",
        "            delta = 1 # Layer removal not allowed\n",
        "        if delta == -1:\n",
        "            # Remove layer.\n",
        "            if layer_num == 0 and first_dense == 1:\n",
        "                layer_num += 1 # Require >= 1 non-dense layer\n",
        "            if layer_num == first_dense and layer_num == last_layer:\n",
        "                layer_num -= 1 # Require >= 1 dense layer\n",
        "            del model_definition[layer_num]\n",
        "        else:\n",
        "            # Add layer.\n",
        "            if layer_num < first_dense:\n",
        "                layer_type = layer_types[np.random.randint(1, 3)]\n",
        "            else:\n",
        "                layer_type = 'DENSE'\n",
        "            layer = generate_random_layer(layer_type)\n",
        "            model_definition.insert(layer_num, layer)\n",
        "    else:\n",
        "        # Tweak parameter.\n",
        "        layer = model_definition[layer_num]\n",
        "        layer_type = layer['layer_type']\n",
        "        params = layer_params[layer_type]\n",
        "        param = params[np.random.randint(0, len(params))]\n",
        "        current_val = layer[param]\n",
        "        values = param_values[param]\n",
        "        index = values.index(current_val)\n",
        "        max_index = len(values)\n",
        "        new_val = values[(index + delta) % max_index]\n",
        "        layer[param] = new_val"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0UxDeJxgaXIh"
      },
      "source": [
        "# Helper method for evolutionary algorithm.\n",
        "def cross_over(parents):\n",
        "    # Pick bottom half of one and top half of the other.\n",
        "    # If model is small, randomly stack top or bottom from both.\n",
        "    bottoms = [[], []]\n",
        "    tops = [[], []]\n",
        "    for i, model in enumerate(parents):\n",
        "        for layer in model:\n",
        "            if layer['layer_type'] != 'DENSE':\n",
        "                bottoms[i].append(copy.deepcopy(layer))\n",
        "            else:\n",
        "                tops[i].append(copy.deepcopy(layer))\n",
        "\n",
        "    i = np.random.randint(0, 2)\n",
        "    if (i == 1 and compute_weight_count(parents[0]) +\n",
        "        compute_weight_count(parents[1]) < MAX_MODEL_SIZE):\n",
        "        i = np.random.randint(0, 2)\n",
        "        new_model = bottoms[i] + bottoms[(i+1)%2]\n",
        "        i = np.random.randint(0, 2)\n",
        "        new_model = new_model + tops[i] + tops[(i+1)%2]\n",
        "    else:\n",
        "        i = np.random.randint(0, 2)\n",
        "        new_model = bottoms[i] + tops[(i+1)%2]\n",
        "    return new_model\n",
        "\n",
        "# Evolutionary algorithm.\n",
        "np.random.seed(7)\n",
        "\n",
        "# Generate initial population of models.\n",
        "population = []\n",
        "for i in range(POPULATION_SIZE):\n",
        "    valid_model = False\n",
        "    while(valid_model == False):\n",
        "        model_definition = generate_model_definition()\n",
        "        acc = create_and_evaluate_model(model_definition)\n",
        "        if acc > 0.0:\n",
        "            valid_model = True\n",
        "    population.append((acc, model_definition))\n",
        "\n",
        "# Evolve population.\n",
        "generations = int(CANDIDATE_EVALUATIONS / POPULATION_SIZE) - 1\n",
        "for i in range(generations):\n",
        "    # Generate new individuals.\n",
        "    print('Generation number: ', i)\n",
        "    for j in range(POPULATION_SIZE):\n",
        "        valid_model = False\n",
        "        while(valid_model == False):\n",
        "            rand = np.random.rand()\n",
        "            parents = random.sample(\n",
        "                population[:POPULATION_SIZE], 2)\n",
        "            parents = [parents[0][1], parents[1][1]]\n",
        "            if rand < 0.5:\n",
        "                child = copy.deepcopy(parents[0])\n",
        "                tweak_model(child)\n",
        "            elif rand < 0.75:\n",
        "                child = cross_over(parents)\n",
        "            else:\n",
        "                child = cross_over(parents)\n",
        "                tweak_model(child)\n",
        "            acc = create_and_evaluate_model(child)\n",
        "            if acc > 0.0:\n",
        "                valid_model = True\n",
        "        population.append((acc, child))\n",
        "    # Randomly select fit individuals.\n",
        "    population.sort(key=lambda x:x[0])\n",
        "    print('Evolution, best accuracy: %5.2f' %population[-1][0])\n",
        "    top = np.int(np.ceil(0.2*len(population)))\n",
        "    bottom = np.int(np.ceil(0.3*len(population)))\n",
        "    top_individuals = population[-top:]\n",
        "    remaining = np.int(len(population)/2) - len(top_individuals)\n",
        "    population = random.sample(population[bottom:-top],\n",
        "                               remaining) + top_individuals\n",
        "\n",
        "best_model = population[-1][1]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S-F3bR-daZjr"
      },
      "source": [
        "# Evaluate final model for larger number of epochs.\n",
        "model = create_model(best_model)\n",
        "model.summary()\n",
        "model.compile(loss='categorical_crossentropy',\n",
        "              optimizer='adam', metrics=['accuracy'])\n",
        "history = model.fit(\n",
        "    train_images, train_labels, validation_data =\n",
        "    (test_images, test_labels), epochs=FINAL_EPOCHS, batch_size=64,\n",
        "    verbose=2, shuffle=True)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}
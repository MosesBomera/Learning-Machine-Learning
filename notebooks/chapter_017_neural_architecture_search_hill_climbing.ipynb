{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "chapter_017_neural_architecture_search_hill_climbing.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3sqzGVhnWCnZ"
      },
      "source": [
        "# Neural Architecture Search"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PvWuW0TsV6DA"
      },
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Lambda\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.layers import Flatten\n",
        "from tensorflow.keras.layers import Reshape\n",
        "from tensorflow.keras.layers import Conv2D\n",
        "from tensorflow.keras.layers import Dropout\n",
        "from tensorflow.keras.layers import MaxPooling2D\n",
        "import numpy as np\n",
        "import logging\n",
        "import copy\n",
        "import random\n",
        "tf.get_logger().setLevel(logging.ERROR)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZCN1PT7HWJzV"
      },
      "source": [
        "MAX_MODEL_SIZE = 500000\n",
        "CANDIDATE_EVALUATIONS = 500\n",
        "EVAL_EPOCHS = 3\n",
        "FINAL_EPOCHS = 20"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OgGgzTdLWpCh"
      },
      "source": [
        "layer_types = ['DENSE', 'CONV2D', 'MAXPOOL2D']\n",
        "param_values = dict([('size', [16, 64, 256, 1024, 4096]),\n",
        "                ('activation', ['relu', 'tanh', 'elu']),\n",
        "                ('kernel_size', [(1, 1), (2, 2), (3, 3), (4, 4)]),\n",
        "                ('stride', [(1, 1), (2, 2), (3, 3), (4, 4)]),\n",
        "                ('dropout', [0.0, 0.4, 0.7, 0.9])])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9lXIU-frWsDc"
      },
      "source": [
        "layer_params = dict([('DENSE', ['size', 'activation', 'dropout']),\n",
        "                     ('CONV2D', ['size', 'activation',\n",
        "                                 'kernel_size', 'stride',\n",
        "                                 'dropout']),\n",
        "                     ('MAXPOOL2D', ['kernel_size', 'stride',\n",
        "                                    'dropout'])])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lURaIRFyWuxX"
      },
      "source": [
        "# Load dataset.\n",
        "cifar_dataset = keras.datasets.cifar10\n",
        "(train_images, train_labels), (test_images,\n",
        "                    test_labels) = cifar_dataset.load_data()\n",
        "\n",
        "# Standardize dataset.\n",
        "mean = np.mean(train_images)\n",
        "stddev = np.std(train_images)\n",
        "train_images = (train_images - mean) / stddev\n",
        "test_images = (test_images - mean) / stddev\n",
        "\n",
        "# Change labels to one-hot.\n",
        "train_labels = to_categorical(train_labels,\n",
        "                              num_classes=10)\n",
        "test_labels = to_categorical(test_labels,\n",
        "                             num_classes=10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bm15BYzwWwrV"
      },
      "source": [
        "def generate_random_layer(layer_type):\n",
        "    layer = {}\n",
        "    layer['layer_type'] = layer_type\n",
        "    params = layer_params[layer_type]\n",
        "    for param in params:\n",
        "        values = param_values[param]\n",
        "        layer[param] = values[np.random.randint(0, len(values))]\n",
        "    return layer"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FZKw2NBNW9ns"
      },
      "source": [
        "def generate_model_definition():\n",
        "    layer_count = np.random.randint(2, 9)\n",
        "    non_dense_count = np.random.randint(1, layer_count)\n",
        "    layers = []\n",
        "    for i in range(layer_count):\n",
        "        if i < non_dense_count:\n",
        "            layer_type = layer_types[np.random.randint(1, 3)]\n",
        "            layer = generate_random_layer(layer_type)\n",
        "        else:\n",
        "            layer = generate_random_layer('DENSE')\n",
        "        layers.append(layer)\n",
        "    return layers"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qpp1S6UqXCrg"
      },
      "source": [
        "def compute_weight_count(layers):\n",
        "    last_shape = (32, 32, 3)\n",
        "    total_weights = 0\n",
        "    for layer in layers:\n",
        "        layer_type = layer['layer_type']\n",
        "        if layer_type == 'DENSE':\n",
        "            size = layer['size']\n",
        "            weights = size * (np.prod(last_shape) + 1)\n",
        "            last_shape = (layer['size'])\n",
        "        else:\n",
        "            stride = layer['stride']\n",
        "            if layer_type == 'CONV2D':\n",
        "                size = layer['size']\n",
        "                kernel_size = layer['kernel_size']\n",
        "                weights = size * ((np.prod(kernel_size) *\n",
        "                                   last_shape[2]) + 1)\n",
        "                last_shape = (np.ceil(last_shape[0]/stride[0]),\n",
        "                              np.ceil(last_shape[1]/stride[1]),\n",
        "                              size)\n",
        "            elif layer_type == 'MAXPOOL2D':\n",
        "                weights = 0\n",
        "                last_shape = (np.ceil(last_shape[0]/stride[0]),\n",
        "                              np.ceil(last_shape[1]/stride[1]),\n",
        "                              last_shape[2])\n",
        "        total_weights += weights\n",
        "    total_weights += ((np.prod(last_shape) + 1) * 10)\n",
        "    return total_weights"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7EZ_Np4kXFit"
      },
      "source": [
        "# Methods to create and evaluate model based on model definition.\n",
        "def add_layer(model, params, prior_type):\n",
        "    layer_type = params['layer_type']\n",
        "    if layer_type == 'DENSE':\n",
        "        if prior_type != 'DENSE':\n",
        "            model.add(Flatten())\n",
        "        size = params['size']\n",
        "        act = params['activation']\n",
        "        model.add(Dense(size, activation=act))\n",
        "    elif layer_type == 'CONV2D':\n",
        "        size = params['size']\n",
        "        act = params['activation']\n",
        "        kernel_size = params['kernel_size']\n",
        "        stride = params['stride']\n",
        "        model.add(Conv2D(size, kernel_size, activation=act,\n",
        "                         strides=stride, padding='same'))\n",
        "    elif layer_type == 'MAXPOOL2D':\n",
        "        kernel_size = params['kernel_size']\n",
        "        stride = params['stride']\n",
        "        model.add(MaxPooling2D(pool_size=kernel_size,\n",
        "                               strides=stride, padding='same'))\n",
        "    dropout = params['dropout']\n",
        "    if(dropout > 0.0):\n",
        "        model.add(Dropout(dropout))\n",
        "\n",
        "def create_model(layers):\n",
        "    tf.keras.backend.clear_session()\n",
        "    model = Sequential()\n",
        "    model.add(Lambda(lambda x: x, input_shape=(32, 32, 3)))\n",
        "    prev_layer = 'LAMBDA' # Dummy layer to set input_shape\n",
        "    for layer in layers:\n",
        "        add_layer(model, layer, prev_layer)\n",
        "        prev_layer = layer['layer_type']\n",
        "    model.add(Dense(10, activation='softmax'))\n",
        "    model.compile(loss='categorical_crossentropy',\n",
        "                  optimizer='adam', metrics=['accuracy'])\n",
        "    return model\n",
        "\n",
        "def create_and_evaluate_model(model_definition):\n",
        "    weight_count = compute_weight_count(model_definition)\n",
        "    if weight_count > MAX_MODEL_SIZE:\n",
        "        return 0.0\n",
        "    model = create_model(model_definition)\n",
        "    history = model.fit(train_images, train_labels,\n",
        "                        validation_data=(test_images, test_labels),\n",
        "                        epochs=EVAL_EPOCHS, batch_size=64,\n",
        "                        verbose=2, shuffle=False)\n",
        "    acc = history.history['val_accuracy'][-1]\n",
        "    print('Size: ', weight_count)\n",
        "    print('Accuracy: %5.2f' %acc)\n",
        "    return acc"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pkVRTVpRXNjV"
      },
      "source": [
        "# Pure random search.\n",
        "np.random.seed(7)\n",
        "val_accuracy = 0.0\n",
        "for i in range(CANDIDATE_EVALUATIONS):\n",
        "    valid_model = False\n",
        "    while(valid_model == False):\n",
        "        model_definition = generate_model_definition()\n",
        "        acc = create_and_evaluate_model(model_definition)\n",
        "        if acc > 0.0:\n",
        "            valid_model = True\n",
        "    if acc > val_accuracy:\n",
        "        best_model = model_definition\n",
        "        val_accuracy = acc\n",
        "    print('Random search, best accuracy: %5.2f' %val_accuracy)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "To4cFJYlXTQ6"
      },
      "source": [
        "# Helper method for hill climbing and evolutionary algorithm.\n",
        "def tweak_model(model_definition):\n",
        "    layer_num = np.random.randint(0, len(model_definition))\n",
        "    last_layer = len(model_definition) - 1\n",
        "    for first_dense, layer in enumerate(model_definition):\n",
        "        if layer['layer_type'] == 'DENSE':\n",
        "            break\n",
        "    if np.random.randint(0, 2) == 1:\n",
        "        delta = 1\n",
        "    else:\n",
        "        delta = -1\n",
        "    if np.random.randint(0, 2) == 1:\n",
        "        # Add/remove layer.\n",
        "        if len(model_definition) < 3:\n",
        "            delta = 1 # Layer removal not allowed\n",
        "        if delta == -1:\n",
        "            # Remove layer.\n",
        "            if layer_num == 0 and first_dense == 1:\n",
        "                layer_num += 1 # Require >= 1 non-dense layer\n",
        "            if layer_num == first_dense and layer_num == last_layer:\n",
        "                layer_num -= 1 # Require >= 1 dense layer\n",
        "            del model_definition[layer_num]\n",
        "        else:\n",
        "            # Add layer.\n",
        "            if layer_num < first_dense:\n",
        "                layer_type = layer_types[np.random.randint(1, 3)]\n",
        "            else:\n",
        "                layer_type = 'DENSE'\n",
        "            layer = generate_random_layer(layer_type)\n",
        "            model_definition.insert(layer_num, layer)\n",
        "    else:\n",
        "        # Tweak parameter.\n",
        "        layer = model_definition[layer_num]\n",
        "        layer_type = layer['layer_type']\n",
        "        params = layer_params[layer_type]\n",
        "        param = params[np.random.randint(0, len(params))]\n",
        "        current_val = layer[param]\n",
        "        values = param_values[param]\n",
        "        index = values.index(current_val)\n",
        "        max_index = len(values)\n",
        "        new_val = values[(index + delta) % max_index]\n",
        "        layer[param] = new_val\n",
        "\n",
        "# Hill climbing, starting from best model from random search.\n",
        "model_definition = best_model\n",
        "\n",
        "for i in range(CANDIDATE_EVALUATIONS):\n",
        "    valid_model = False\n",
        "    while(valid_model == False):\n",
        "        old_model_definition = copy.deepcopy(model_definition)\n",
        "        tweak_model(model_definition)\n",
        "        acc = create_and_evaluate_model(model_definition)\n",
        "        if acc > 0.0:\n",
        "            valid_model = True\n",
        "        else:\n",
        "            model_definition = old_model_definition\n",
        "    if acc > val_accuracy:\n",
        "        best_model = copy.deepcopy(model_definition)\n",
        "        val_accuracy = acc\n",
        "    else:\n",
        "        model_definition = old_model_definition\n",
        "    print('Hill climbing, best accuracy: %5.2f' %val_accuracy)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pma8fp3dX3KL"
      },
      "source": [
        "# Evaluate final model for larger number of epochs.\n",
        "model = create_model(best_model)\n",
        "model.summary()\n",
        "model.compile(loss='categorical_crossentropy',\n",
        "              optimizer='adam', metrics=['accuracy'])\n",
        "history = model.fit(\n",
        "    train_images, train_labels, validation_data =\n",
        "    (test_images, test_labels), epochs=FINAL_EPOCHS, batch_size=64,\n",
        "    verbose=2, shuffle=True)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}
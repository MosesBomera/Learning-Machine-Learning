{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "chapter_016.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t957xVAAKg17"
      },
      "source": [
        "# One-to-Many Network for Image Captioning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZKUn9EHbK6o-"
      },
      "source": [
        "This notebook uses the annotated COCO dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lcq4LhDmKMkg"
      },
      "source": [
        "# Data preparation\n",
        "# Create the data directories.\n",
        "!mkdir data\n",
        "!mkdir data/coco\n",
        "# Download the annotations.\n",
        "!wget http://images.cocodataset.org/annotations/annotations_trainval2014.zip\n",
        "# Unzip annotations to the coco folder\n",
        "!unzip annotations_trainval2014.zip -d data/coco\n",
        "# Delete the zip file\n",
        "!rm annotations_trainval2014.zip\n",
        "# Create output folder\n",
        "!mkdir output\n",
        "!mkdir output/feature_vectors"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7rkECTzDLeUa"
      },
      "source": [
        "# Download the data itself\n",
        "!wget http://images.cocodataset.org/zips/train2014.zip\n",
        "# Unzip the dataset into data/coco\n",
        "!unzip train2014.zip -d data/coco\n",
        "# Free up some space.\n",
        "!rm train2014.zip"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D5lI78QRQgMz"
      },
      "source": [
        "TENSORFLOW"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X_u40OxeO0qm"
      },
      "source": [
        "# Imports\n",
        "import json\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.applications import VGG19\n",
        "from tensorflow.keras.applications.vgg19 import preprocess_input\n",
        "from tensorflow.keras.preprocessing.image import load_img\n",
        "from tensorflow.keras.preprocessing.image import img_to_array\n",
        "import pickle\n",
        "import gzip\n",
        "import logging\n",
        "from pathlib import Path\n",
        "from tqdm import tqdm\n",
        "tf.get_logger().setLevel(logging.ERROR)\n",
        "\n",
        "# Set directories.\n",
        "TRAINING_FILE_DIR = Path('data/coco')\n",
        "OUTPUT_FILE_DIR = Path('output/feature_vectors')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7hrQj4i6Sq63"
      },
      "source": [
        "# Preprocessing the image captions.\n",
        "# Open the file using a context manager.\n",
        "with open(TRAINING_FILE_DIR / 'annotations/captions_train2014.json') as captions:\n",
        "  data = json.load(captions)\n",
        "\n",
        "image_dict = {}\n",
        "# Get the image filenames.\n",
        "for image in data['images']:\n",
        "  image_dict[image['id']] = [image['file_name']]\n",
        "# Get the annotations, each id is assigned a list, the first element\n",
        "# represents the filename, the subsequent elements are captions.\n",
        "for annotations in data['annotations']:\n",
        "  image_dict[annotations['image_id']].append(annotations['caption'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lwc0HtGsUuDA"
      },
      "source": [
        "# The model implements an encoder-decoder architecture.\n",
        "# The encoder is a pretrained VGG19 model.\n",
        "model = VGG19(weights='imagenet')\n",
        "# model.summary()\n",
        "# Find the name of the last network, -> block5_conv4\n",
        "encoder = Model(inputs=model.input, \n",
        "                  outputs=model.get_layer('block5_conv4').output)\n",
        "encoder.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Azys7TaCVeNx"
      },
      "source": [
        "# To save on computation, the VGG19 won't be retrained,\n",
        "# instead for each image, an output vector of the model's\n",
        "# forward pass will be stored.\n",
        "for idx, key in tqdm(enumerate(image_dict.keys()), desc='Progress: '):\n",
        "  item = image_dict.get(key)\n",
        "  filename = TRAINING_FILE_DIR / 'train2014' / item[0]\n",
        "  # Determine dimensions.\n",
        "  image = load_img(filename)\n",
        "  width = image.size[0]\n",
        "  height = image.size[1]\n",
        "  # Resize so shortest side is 256 pixels.\n",
        "  if height > width:\n",
        "    image = load_img(filename, target_size=(int(height/width*256), 256))\n",
        "  else:\n",
        "    image = load_img(filename, target_size=(256, int(width/height*256)))\n",
        "  width = image.size[0]\n",
        "  height = image.size[1]\n",
        "  image_np = img_to_array(image)\n",
        "  # Crop to center 224x224 region.\n",
        "  h_start = int((height-224)/2)\n",
        "  w_start = int((width-224)/2)\n",
        "  image_np = image_np[\n",
        "    h_start:h_start+224,\n",
        "    w_start:w_start+224\n",
        "  ]\n",
        "  # Rearrange array to have one more\n",
        "  # dimension representing batch size = 1.\n",
        "  image_np = np.expand_dims(image_np, axis=0)\n",
        "  # Call model and save resultin tensor to disk.\n",
        "  X = preprocess_input(image_np)\n",
        "  y = encoder.predict(X)\n",
        "  save_filename = OUTPUT_FILE_DIR / f'{item[0]}.pickle.gzip'\n",
        "  pickle_file = gzip.open(save_filename, 'wb')\n",
        "  pickle.dump(y[0], pickle_file)\n",
        "  pickle_file.close() \n",
        "\n",
        "# Save the dictionary containing captions and filenames.\n",
        "save_filename = OUTPUT_FILE_DIR + 'caption_file.pickle.gz'\n",
        "pickle_file = gzip.open(save_filename, 'wb')\n",
        "pickle.dump(image_dict, pickle_file)\n",
        "pickle_file.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5zuCqqw5fDmo"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}
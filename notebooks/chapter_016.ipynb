{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "chapter_016.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t957xVAAKg17"
      },
      "source": [
        "# One-to-Many Network for Image Captioning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZKUn9EHbK6o-"
      },
      "source": [
        "This notebook uses the annotated COCO dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lcq4LhDmKMkg"
      },
      "source": [
        "# Data preparation\n",
        "# Create the data directories.\n",
        "!mkdir data\n",
        "!mkdir data/coco\n",
        "# Download the annotations.\n",
        "!wget http://images.cocodataset.org/annotations/annotations_trainval2014.zip\n",
        "# Unzip annotations to the coco folder\n",
        "!unzip annotations_trainval2014.zip -d data/coco\n",
        "# Delete the zip file\n",
        "!rm annotations_trainval2014.zip\n",
        "# Create output folder\n",
        "!mkdir output\n",
        "!mkdir output/feature_vectors"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7rkECTzDLeUa"
      },
      "source": [
        "# Download the data itself\n",
        "!wget http://images.cocodataset.org/zips/val2014.zip\n",
        "# Unzip the dataset into data/coco\n",
        "!unzip val2014.zip -d data/coco\n",
        "# Free up some space.\n",
        "!rm val2014.zip"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D5lI78QRQgMz"
      },
      "source": [
        "TENSORFLOW"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X_u40OxeO0qm"
      },
      "source": [
        "# Imports\n",
        "import json\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.applications import VGG19\n",
        "from tensorflow.keras.applications.vgg19 import preprocess_input\n",
        "from tensorflow.keras.preprocessing.image import load_img\n",
        "from tensorflow.keras.preprocessing.image import img_to_array\n",
        "import pickle\n",
        "import gzip\n",
        "import logging\n",
        "from pathlib import Path\n",
        "from tqdm import tqdm\n",
        "tf.get_logger().setLevel(logging.ERROR)\n",
        "\n",
        "# Set directories.\n",
        "TRAINING_FILE_DIR = Path('data/coco')\n",
        "OUTPUT_FILE_DIR = Path('output/feature_vectors')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7hrQj4i6Sq63"
      },
      "source": [
        "# Preprocessing the image captions.\n",
        "# Open the file using a context manager.\n",
        "with open(TRAINING_FILE_DIR / 'annotations/captions_val2014.json') as captions:\n",
        "  data = json.load(captions)\n",
        "\n",
        "image_dict = {}\n",
        "# Get the image filenames.\n",
        "for image in data['images']:\n",
        "  image_dict[image['id']] = [image['file_name']]\n",
        "# Get the annotations, each id is assigned a list, the first element\n",
        "# represents the filename, the subsequent elements are captions.\n",
        "for annotations in data['annotations']:\n",
        "  image_dict[annotations['image_id']].append(annotations['caption'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q7NtXz2tZCWI"
      },
      "source": [
        "image_dict[391895]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lwc0HtGsUuDA"
      },
      "source": [
        "# The model implements an encoder-decoder architecture.\n",
        "# The encoder is a pretrained VGG19 model.\n",
        "model = VGG19(weights='imagenet')\n",
        "# model.summary()\n",
        "# Find the name of the last network, -> block5_conv4\n",
        "encoder = Model(inputs=model.input, \n",
        "                  outputs=model.get_layer('block5_conv4').output)\n",
        "encoder.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Azys7TaCVeNx"
      },
      "source": [
        "# To save on computation, the VGG19 won't be retrained,\n",
        "# instead for each image, an output vector of the model's\n",
        "# forward pass will be stored.\n",
        "# Cap the images at 15,000 images.\n",
        "i = 0\n",
        "for idx, key in tqdm(enumerate(image_dict.keys()), desc='Progress: '):\n",
        "  item = image_dict.get(key)\n",
        "  filename = TRAINING_FILE_DIR / 'val2014' / item[0]\n",
        "  # Determine dimensions.\n",
        "  image = load_img(filename)\n",
        "  width = image.size[0]\n",
        "  height = image.size[1]\n",
        "  # Resize so shortest side is 256 pixels.\n",
        "  if height > width:\n",
        "    image = load_img(filename, target_size=(int(height/width*256), 256))\n",
        "  else:\n",
        "    image = load_img(filename, target_size=(256, int(width/height*256)))\n",
        "  width = image.size[0]\n",
        "  height = image.size[1]\n",
        "  image_np = img_to_array(image)\n",
        "  # Crop to center 224x224 region.\n",
        "  h_start = int((height-224)/2)\n",
        "  w_start = int((width-224)/2)\n",
        "  image_np = image_np[\n",
        "    h_start:h_start+224,\n",
        "    w_start:w_start+224\n",
        "  ]\n",
        "  # Rearrange array to have one more\n",
        "  # dimension representing batch size = 1.\n",
        "  image_np = np.expand_dims(image_np, axis=0)\n",
        "  # Call model and save resultin tensor to disk.\n",
        "  X = preprocess_input(image_np)\n",
        "  y = encoder.predict(X)\n",
        "  save_filename = OUTPUT_FILE_DIR / f'{item[0]}.pickle.gzip'\n",
        "  pickle_file = gzip.open(save_filename, 'wb')\n",
        "  pickle.dump(y[0], pickle_file)\n",
        "  pickle_file.close() \n",
        "  if i == 15000:\n",
        "    break # Stop here for learning purposes.\n",
        "  i += 1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mhCOU9_dkcuc"
      },
      "source": [
        "# Save the dictionary containing captions and filenames.\n",
        "save_filename = OUTPUT_FILE_DIR / 'caption_file.pickle.gz'\n",
        "pickle_file = gzip.open(save_filename, 'wb')\n",
        "pickle.dump(image_dict, pickle_file)\n",
        "pickle_file.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1JEpp9qkKCFg"
      },
      "source": [
        "### IMAGE CAPTIONING MODEL"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RC2Y1cAZKElG"
      },
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras.layers import Input\n",
        "from tensorflow.keras.layers import Embedding\n",
        "from tensorflow.keras.layers import LSTM\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.layers import Attention\n",
        "from tensorflow.keras.layers import Concatenate\n",
        "from tensorflow.keras.layers import GlobalAveragePooling2D\n",
        "from tensorflow.keras.layers import Reshape\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.text import text_to_word_sequence\n",
        "from tensorflow.keras.applications import VGG19\n",
        "from tensorflow.keras.applications.vgg19 import preprocess_input\n",
        "from tensorflow.keras.preprocessing.image import load_img\n",
        "from tensorflow.keras.preprocessing.image import img_to_array\n",
        "from tensorflow.keras.utils import Sequence\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "import pickle\n",
        "import gzip\n",
        "import logging\n",
        "tf.get_logger().setLevel(logging.ERROR)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YmOjugHBQFAW"
      },
      "source": [
        "# Initialization Statements\n",
        "EPOCHS = 20\n",
        "BATCH_SIZE = 128\n",
        "MAX_WORDS = 10000\n",
        "LAYER_SIZE = 256\n",
        "READ_IMAGES = 15000\n",
        "EMBEDDING_WIDTH = 128\n",
        "OOV_WORD = 'UNK'\n",
        "PAD_INDEX = 0\n",
        "OOV_INDEX = 1\n",
        "START_INDEX = MAX_WORDS - 2\n",
        "STOP_INDEX = MAX_WORDS - 1\n",
        "MAX_LENGTH = 60\n",
        "TRAINING_FILE_DIR = OUTPUT_FILE_DIR\n",
        "# TEST_FILE_DIR = Path('test_images')\n",
        "# TEST_IMAGES = ['boat.jpg', 'cat.jpg', 'table.jpg', 'bird.jpg']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VHKdslxqRBMO"
      },
      "source": [
        "# Read in the captions for the vectorized image representations.\n",
        "def read_training_file(file_name, max_len):\n",
        "  pickle_file = gzip.open(file_name, 'rb')\n",
        "  image_dict = pickle.load(pickle_file)\n",
        "  pickle_file.close()\n",
        "  image_paths = []\n",
        "  dest_word_sequences = []\n",
        "  for idx, key in enumerate(image_dict):\n",
        "    if idx == READ_IMAGES:\n",
        "      break\n",
        "    image_item = image_dict[key]\n",
        "    image_paths.append(image_item[0])\n",
        "    caption = image_item[1]\n",
        "    word_sequence = text_to_word_sequence(caption)\n",
        "    dest_word_sequence = word_sequence[0:max_len]\n",
        "    dest_word_sequences.append(dest_word_sequence)\n",
        "  return image_paths, dest_word_sequences"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9W8n3yqmdLbo"
      },
      "source": [
        "# Functions to tokenize and un-tokenize sequences.\n",
        "def tokenize(sequences):\n",
        "  tokenizer = Tokenizer(num_words=MAX_WORDS-2, oov_token=OOV_WORD)\n",
        "  tokenizer.fit_on_texts(sequences)\n",
        "  token_sequences = tokenizer.texts_to_sequences(sequences)\n",
        "  return tokenizer, token_sequences\n",
        "\n",
        "def tokens_to_words(tokenizer, seq):\n",
        "  word_seq = []\n",
        "  for index in seq:\n",
        "    if index == PAD_INDEX:\n",
        "      word_seq.append('PAD')\n",
        "    elif index == OOV_INDEX:\n",
        "      word_seq.append(OOV_WORD)\n",
        "    elif index == START_INDEX:\n",
        "      word_seq.append('START')\n",
        "    elif index == STOP_INDEX:\n",
        "      word_seq.append('STOP')\n",
        "    else:\n",
        "      word_seq.append(tokenizer.sequences_to_texts([[index]])[0])\n",
        "    print(word_seq)\n",
        "\n",
        "# Read files.\n",
        "image_paths, dest_seq = read_training_file(TRAINING_FILE_DIR / 'caption_file.pickle.gz', MAX_LENGTH)\n",
        "dest_tokenizer, dest_token_seq = tokenize(dest_seq)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3Sa_k6ejnotg"
      },
      "source": [
        "# image_paths"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qgS9SONngzSV"
      },
      "source": [
        "# Sequence class to create batches on the fly.\n",
        "class ImageCaptionSequence(Sequence):\n",
        "  def __init__(self, image_paths, dest_input_data, dest_target_data, batch_size):\n",
        "    self.image_paths = image_paths\n",
        "    self.dest_input_data = dest_input_data\n",
        "    self.dest_target_data = dest_target_data\n",
        "    self.batch_size = batch_size\n",
        "\n",
        "  def __len__(self):\n",
        "    \"\"\"The number of batches in the Sequence.\"\"\"\n",
        "    return int(np.ceil(len(self.dest_input_data) / float(self.batch_size)))\n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "    batch_x0 = self.image_paths[idx*self.batch_size:(idx + 1)*self.batch_size]\n",
        "    batch_x1 = self.dest_input_data[idx*self.batch_size:(idx+1)*self.batch_size]\n",
        "    batch_y = self.dest_target_data[idx*self.batch_size:(idx+1)*self.batch_size]\n",
        "    image_features = []\n",
        "    for image_id in batch_x0:\n",
        "      file_name = TRAINING_FILE_DIR / f\"{image_id}.pickle.gzip\"\n",
        "      pickle_file = gzip.open(file_name, 'rb')\n",
        "      feature_vector = pickle.load(pickle_file)\n",
        "      pickle_file.close()\n",
        "      image_features.append(feature_vector)\n",
        "      return [np.array(image_features), np.array(batch_x1), np.array(batch_y)]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hl-9lvD2lca3"
      },
      "source": [
        "# Prepare training data.\n",
        "dest_target_token_seq = [x + [STOP_INDEX]  for x in dest_token_seq]\n",
        "dest_input_token_seq = [[START_INDEX] + x for x in dest_target_token_seq]\n",
        "dest_input_data = pad_sequences(dest_input_token_seq, padding='post')\n",
        "dest_target_data = pad_sequences(dest_target_token_seq, padding='post', maxlen=len(dest_input_data[0]))\n",
        "image_sequence = ImageCaptionSequence(\n",
        "    image_paths, dest_input_data, dest_target_data, BATCH_SIZE\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2L6NvW-Dnfjl"
      },
      "source": [
        "# Build encoder model\n",
        "# Input is feature vector\n",
        "feature_vector_input = Input(shape=(14, 14, 512))\n",
        "\n",
        "# Create the encoder layers.\n",
        "enc_mean_layer = GlobalAveragePooling2D()\n",
        "enc_layer_h = Dense(LAYER_SIZE)\n",
        "enc_layer_c = Dense(LAYER_SIZE)\n",
        "\n",
        "# Connect the encoding layers.\n",
        "enc_mean_layer_output = enc_mean_layer(feature_vector_input)\n",
        "enc_layer_h_outputs = enc_layer_h(enc_mean_layer_output)\n",
        "enc_layer_c_outputs = enc_layer_c(enc_mean_layer_output)\n",
        "\n",
        "# Organize the output state for encoder layers.\n",
        "enc_layer_outputs = [enc_layer_h_outputs, enc_layer_c_outputs]\n",
        "\n",
        "# Build the model\n",
        "enc_model_top = Model(feature_vector_input, enc_layer_outputs)\n",
        "enc_model_top.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dltsv_I50c7q"
      },
      "source": [
        "# The decoder model \n",
        "# Input to the network is feature_vector, image caption \n",
        "# sequence, and intermediate state.\n",
        "dec_feature_vector_input = Input(shape=(14, 14, 512))\n",
        "dec_embedding_input = Input(shape=(None, ))\n",
        "dec_layer1_state_input_h = Input(shape=(LAYER_SIZE,))\n",
        "dec_layer1_state_input_c = Input(shape=(LAYER_SIZE,))\n",
        "\n",
        "# Create the decoder layers.\n",
        "dec_reshape_layer = Reshape((196, 512), input_shape=(14,14,512))\n",
        "dec_attention_layer = Attention()\n",
        "dec_query_layer = Dense(512)\n",
        "dec_embedding_layer = Embedding(\n",
        "    output_dim=EMBEDDING_WIDTH, input_dim=MAX_WORDS, mask_zero=False)\n",
        "dec_layer1 = LSTM(LAYER_SIZE, return_state=True, return_sequences=True)\n",
        "dec_concat_layer = Concatenate()\n",
        "dec_layer2 = Dense(MAX_WORDS, activation='softmax')\n",
        "\n",
        "# Connect the decoder layers.\n",
        "dec_embedding_layer_outputs = dec_embedding_layer(\n",
        "    dec_embedding_input)\n",
        "dec_reshape_layer_outputs = dec_reshape_layer(\n",
        "    dec_feature_vector_input)\n",
        "dec_layer1_outputs, dec_layer1_state_h, dec_layer1_state_c = \\\n",
        "    dec_layer1(dec_embedding_layer_outputs, initial_state=[\n",
        "        dec_layer1_state_input_h, dec_layer1_state_input_c])\n",
        "dec_query_layer_outputs = dec_query_layer(dec_layer1_outputs)\n",
        "dec_attention_layer_outputs = dec_attention_layer(\n",
        "    [dec_query_layer_outputs, dec_reshape_layer_outputs])\n",
        "dec_layer2_inputs = dec_concat_layer(\n",
        "    [dec_layer1_outputs, dec_attention_layer_outputs])\n",
        "dec_layer2_outputs = dec_layer2(dec_layer2_inputs)\n",
        "\n",
        "# Build the model.\n",
        "dec_model = Model([dec_feature_vector_input,\n",
        "                   dec_embedding_input,\n",
        "                   dec_layer1_state_input_h,\n",
        "                   dec_layer1_state_input_c],\n",
        "                  [dec_layer2_outputs, dec_layer1_state_h,\n",
        "                   dec_layer1_state_c])\n",
        "dec_model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3G5xEKtG1GMD"
      },
      "source": [
        "# Build and compile full training model.\n",
        "# We do not use the state output when training.\n",
        "train_feature_vector_input = Input(shape=(14, 14, 512))\n",
        "train_dec_embedding_input = Input(shape=(None, ))\n",
        "intermediate_state = enc_model_top(train_feature_vector_input)\n",
        "train_dec_output, _, _ = dec_model([train_feature_vector_input,\n",
        "                                    train_dec_embedding_input] +\n",
        "                                    intermediate_state)\n",
        "training_model = Model([train_feature_vector_input,\n",
        "                        train_dec_embedding_input],\n",
        "                        [train_dec_output])\n",
        "training_model.compile(loss='sparse_categorical_crossentropy',\n",
        "                       optimizer='adam', metrics =['accuracy'])\n",
        "training_model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dkwdL0Xr4wwN"
      },
      "source": [
        "# Build full encoder model for inference.\n",
        "conv_model = VGG19(weights='imagenet')\n",
        "conv_model_outputs = conv_model.get_layer('block5_conv4').output\n",
        "intermediate_state = enc_model_top(conv_model_outputs)\n",
        "inference_enc_model = Model([conv_model.input],\n",
        "                            intermediate_state\n",
        "                            + [conv_model_outputs])\n",
        "inference_enc_model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LS5pGCBw5A2b"
      },
      "source": [
        "for i in range(EPOCHS): # Train and evaluate model\n",
        "    print('step: ' , i)\n",
        "    history = training_model.fit(image_sequence, epochs=1)\n",
        "    for filename in TEST_IMAGES:\n",
        "        # Determine dimensions.\n",
        "        image = load_img(TEST_FILE_DIR + filename)\n",
        "        width = image.size[0]\n",
        "        height = image.size[1]\n",
        "\n",
        "        # Resize so shortest side is 256 pixels.\n",
        "        if height > width:\n",
        "            image = load_img(\n",
        "                TEST_FILE_DIR + filename,\n",
        "                target_size=(int(height/width*256), 256))\n",
        "        else:\n",
        "            image = load_img(\n",
        "                TEST_FILE_DIR + filename,\n",
        "                target_size=(256, int(width/height*256)))\n",
        "        width = image.size[0]\n",
        "        height = image.size[1]\n",
        "        image_np = img_to_array(image)\n",
        "\n",
        "        # Crop to center 224x224 region.\n",
        "        h_start = int((height-224)/2)\n",
        "        w_start = int((width-224)/2)\n",
        "        image_np = image_np[h_start:h_start+224,\n",
        "                            w_start:w_start+224]\n",
        "\n",
        "        # Run image through encoder.\n",
        "        image_np = np.expand_dims(image_np, axis=0)\n",
        "        x = preprocess_input(image_np)\n",
        "        dec_layer1_state_h, dec_layer1_state_c, feature_vector = \\\n",
        "            inference_enc_model.predict(x, verbose=0)\n",
        "\n",
        "        # Predict sentence word for word.\n",
        "        prev_word_index = START_INDEX\n",
        "        produced_string = ''\n",
        "        pred_seq = []\n",
        "        for j in range(MAX_LENGTH):\n",
        "            x = np.reshape(np.array(prev_word_index), (1, 1))\n",
        "            preds, dec_layer1_state_h, dec_layer1_state_c = \\\n",
        "                dec_model.predict(\n",
        "                    [feature_vector, x, dec_layer1_state_h,\n",
        "                     dec_layer1_state_c], verbose=0)\n",
        "            prev_word_index = np.asarray(preds[0][0]).argmax()\n",
        "            pred_seq.append(prev_word_index)\n",
        "            if prev_word_index == STOP_INDEX:\n",
        "                break\n",
        "        tokens_to_words(dest_tokenizer, pred_seq)\n",
        "        print('\\n\\n')"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}
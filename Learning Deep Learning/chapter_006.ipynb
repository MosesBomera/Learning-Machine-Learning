{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 6. Fully Connected Networks Applied to Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TENSORFLOW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LIBRARIES\n",
    "import tensorflow as tf \n",
    "from tensorflow import keras \n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "import numpy as np \n",
    "import logging\n",
    "tf.get_logger().setLevel(logging.ERROR)\n",
    "\n",
    "EPOCHS = 500\n",
    "BATCH_SIZE = 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read and standardize the data\n",
    "boston_housing = keras.datasets.boston_housing\n",
    "(raw_x_train, y_train), (raw_x_test, y_test) = boston_housing.load_data()\n",
    "x_mean = np.mean(raw_x_train, axis=0)\n",
    "x_stddev = np.std(raw_x_train, axis=0)\n",
    "x_mean, x_stddev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ground truth\n",
    "y_train[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train =(raw_x_train - x_mean) / x_stddev\n",
    "x_test =(raw_x_test - x_mean) / x_stddev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MODEL\n",
    "model = Sequential()\n",
    "model.add(Dense(64, activation='relu', input_shape=[13]))\n",
    "model.add(Dense(64, activation='relu')) # DL\n",
    "model.add(Dense(1, activation='linear'))\n",
    "model.compile(loss=\"mean_squared_error\", optimizer='adam', metrics=['mean_absolute_error'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(\n",
    "    x_train, y_train, \n",
    "    validation_data=(x_test, y_test),\n",
    "    epochs=EPOCHS, batch_size=BATCH_SIZE, \n",
    "    verbose=2, shuffle=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print first 4 predictions.\n",
    "predictions = model.predict(x_test)\n",
    "for i in range(0, 4):\n",
    "    print('Prediction: ', predictions[i],\n",
    "          ', true value: ', y_test[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Why DL? Could linear regression perform better?\n",
    "model = Sequential()\n",
    "model.add(Dense(1, activation='linear', input_shape=[13]))\n",
    "model.compile(loss=\"mean_squared_error\", optimizer=\"adam\", metrics=[\"mean_absolute_error\"])\n",
    "history = model.fit(x_train, y_train, validation_data=(x_test, y_test), verbose=2,\n",
    "            epochs=EPOCHS, batch_size=BATCH_SIZE\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print first 4 predictions.\n",
    "predictions = model.predict(x_test)\n",
    "for i in range(0, 4):\n",
    "    print('Prediction: ', predictions[i],\n",
    "          ', true value: ', y_test[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DEEPER NETWORK WITH REGULARIZATION USING DROPOUT.\n",
    "import tensorflow as tf \n",
    "from tensorflow import keras \n",
    "from tensorflow.keras.models import Sequential \n",
    "from tensorflow.keras.layers import Dense \n",
    "from tensorflow.keras.layers import Dropout \n",
    "\n",
    "import numpy as np \n",
    "import logging \n",
    "tf.get_logger().setLevel(logging.ERROR)\n",
    "\n",
    "EPOCHS = 500 \n",
    "BATCH_SIZE = 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read and standardize the data.\n",
    "boston_housing = keras.datasets.boston_housing\n",
    "(raw_x_train, y_train), (raw_x_test,\n",
    "    y_test) = boston_housing.load_data()\n",
    "x_mean = np.mean(raw_x_train, axis=0)\n",
    "x_stddev = np.std(raw_x_train, axis=0)\n",
    "x_train =(raw_x_train - x_mean) / x_stddev\n",
    "x_test =(raw_x_test - x_mean) / x_stddev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(128, activation='relu', input_shape=[13]))\n",
    "model.add(Dropout(0.3))\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dropout(0.3))\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dropout(0.3))\n",
    "model.add(Dense(1, activation='linear'))\n",
    "model.compile(loss='mean_squared_error', optimizer='adam', metrics=['mean_absolute_error'])\n",
    "model.summary()\n",
    "history = model.fit(x_train, y_train, validation_data=(x_test, y_test), \n",
    "epochs=EPOCHS, batch_size=BATCH_SIZE, verbose=2, shuffle=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print first 4 predictions.\n",
    "predictions = model.predict(x_test)\n",
    "for i in range(0, 4):\n",
    "    print('Prediction: ', predictions[i, 0],\n",
    "          ', true value: ', y_test[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PYTORCH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_boston\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch \n",
    "import torch.nn as nn \n",
    "from torch.utils.data import TensorDataset, DataLoader \n",
    "import numpy as np \n",
    "from utils import train_model\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "EPOCHS = 500\n",
    "BATCH_SIZE = 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The dataset\n",
    "boston_housing = load_boston()\n",
    "data, target = boston_housing.get('data'), boston_housing.get('target')\n",
    "# Data split\n",
    "raw_x_train, raw_x_test, y_train, y_test = train_test_split(\n",
    "    data, target, test_size=0.2, random_state=0)\n",
    "# Convert to same precison as model\n",
    "raw_x_train = raw_x_train.astype(np.float32)\n",
    "raw_x_test = raw_x_test.astype(np.float32)\n",
    "y_train = y_train.astype(np.float32)\n",
    "y_test = y_test.astype(np.float32)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = np.reshape(y_train, (-1, 1))\n",
    "y_test = np.reshape(y_test, (-1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_mean = np.mean(raw_x_train, axis=0)\n",
    "x_stddev = np.std(raw_x_train, axis=0)\n",
    "x_train = (raw_x_train - x_mean) / x_stddev\n",
    "x_test = (raw_x_test - x_mean) / x_stddev\n",
    "\n",
    "# Create Dataset objects.\n",
    "trainset = TensorDataset(torch.from_numpy(x_train),\n",
    "                         torch.from_numpy(y_train))\n",
    "testset = TensorDataset(torch.from_numpy(x_test),\n",
    "                        torch.from_numpy(y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PYTORCH MODEL \n",
    "model = nn.Sequential(\n",
    "    nn.Linear(13, 64),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(64,64),\n",
    "    nn.ReLU(), \n",
    "    nn.Linear(64,1)\n",
    ")\n",
    "\n",
    "# Initialize weights.\n",
    "for module in model.modules():\n",
    "    if isinstance(module, nn.Linear):\n",
    "        nn.init.xavier_uniform_(module.weight)\n",
    "        nn.init.constant_(module.bias, 0.0)\n",
    "\n",
    "# Loss and Optimizer \n",
    "optimizer = torch.optim.Adam(model.parameters())\n",
    "loss_function = nn.MSELoss()\n",
    "\n",
    "# Train model\n",
    "train_model(model, device, EPOCHS, BATCH_SIZE, trainset, testset, optimizer, loss_function, 'mae')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First four predictions.\n",
    "inputs = torch.from_numpy(x_test)\n",
    "inputs = inputs.to(device)\n",
    "outputs = model(inputs)\n",
    "for i in range(0,4):\n",
    "    print(f'Prediction: {outputs.data[i].item():4.2f}', \n",
    "        f', true value: {y_test[i].item():4.2f}'\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DEEPER NETWORK WITH REGULARIZATION USING DROPOUT.\n",
    "from sklearn.datasets import load_boston\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import numpy as np\n",
    "from utils import train_model\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "EPOCHS = 500\n",
    "BATCH_SIZE = 16\n",
    "\n",
    "# Read and standardize the data.\n",
    "boston_housing = load_boston()\n",
    "data = boston_housing.get('data')\n",
    "target = boston_housing.get('target')\n",
    "\n",
    "raw_x_train, raw_x_test, y_train, y_test = train_test_split(\n",
    "    data, target, test_size=0.2, random_state=0)\n",
    "\n",
    "# Convert to same precision as model.\n",
    "raw_x_train = raw_x_train.astype(np.float32)\n",
    "raw_x_test = raw_x_test.astype(np.float32)\n",
    "y_train = y_train.astype(np.float32)\n",
    "y_test = y_test.astype(np.float32)\n",
    "y_train = np.reshape(y_train, (-1, 1))\n",
    "y_test = np.reshape(y_test, (-1, 1))\n",
    "x_mean = np.mean(raw_x_train, axis=0)\n",
    "x_stddev = np.std(raw_x_train, axis=0)\n",
    "x_train = (raw_x_train - x_mean) / x_stddev\n",
    "x_test = (raw_x_test - x_mean) / x_stddev\n",
    "\n",
    "# Create Dataset objects.\n",
    "trainset = TensorDataset(torch.from_numpy(x_train),\n",
    "                         torch.from_numpy(y_train))\n",
    "testset = TensorDataset(torch.from_numpy(x_test),\n",
    "                        torch.from_numpy(y_test))\n",
    "\n",
    "\n",
    "# MODEL \n",
    "model = nn.Sequential(\n",
    "    nn.Linear(13, 128),\n",
    "    nn.ReLU(),\n",
    "    nn.Dropout(0.3),\n",
    "    nn.Linear(128, 128),\n",
    "    nn.ReLU(),\n",
    "    nn.Dropout(0.3),\n",
    "    nn.Linear(128, 64),\n",
    "    nn.ReLU(),\n",
    "    nn.Dropout(0.3),\n",
    "    nn.Linear(64, 1)\n",
    ")\n",
    "\n",
    "# Initialize weights.\n",
    "for module in model.modules():\n",
    "    if isinstance(module, nn.Linear):\n",
    "        nn.init.xavier_uniform_(module.weight)\n",
    "        nn.init.constant_(module.bias, 0.0)\n",
    "\n",
    "# Loss function and optimizer\n",
    "optimizer = torch.optim.Adam(model.parameters())\n",
    "loss_function = nn.MSELoss()\n",
    "\n",
    "# Train model.\n",
    "train_model(model, device, EPOCHS, BATCH_SIZE, trainset, testset,\n",
    "            optimizer, loss_function, 'mae')\n",
    "\n",
    "# Print first 4 predictions.\n",
    "inputs = torch.from_numpy(x_test)\n",
    "inputs = inputs.to(device)\n",
    "outputs = model(inputs)\n",
    "for i in range(0, 4):\n",
    "    print('Prediction: %4.2f' % outputs.data[i].item(),\n",
    "         ', true value: %4.2f' % y_test[i].item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "b0bf3a60f070728456081e0416621110ebcfe2b6b1c96a4ddde8f44cf79d65c9"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
